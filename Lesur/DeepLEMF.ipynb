{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65224c0b-e0eb-4452-857d-aa4f0afd1c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.ndimage import uniform_filter1d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e212166b-85b2-4421-8e3c-a23d6fbba261",
   "metadata": {},
   "source": [
    "## 1- Load the dataset from the simulation\n",
    "\n",
    "This dataset contains horizontally-averaged quantities all stored in a simple text file\n",
    "\n",
    "We first start by defining a couple of useful functions to read the input file and to make time averages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65941396-5116-4314-ad78-b434e047e25c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def readProfile(filename):\n",
    "        \n",
    "    fid=open(filename,\"r\")\n",
    "    \n",
    "    # read the first line to get data names\n",
    "    varnames=fid.readline().split()\n",
    "    fid.close()\n",
    "    \n",
    "    nprof=np.size(varnames);\n",
    "    \n",
    "    # load the bulk of the file\n",
    "    data=np.loadtxt(filename,skiprows=1)\n",
    "    \n",
    "    # store this in our data structure\n",
    "    prof={}\n",
    "    prof['z']=data[0,1:]\n",
    "    prof['t']=data[1::nprof,0]\n",
    "\n",
    "    i=0\n",
    "    for name in varnames:\n",
    "        prof[name]=np.transpose(data[1+i::nprof,1:])\n",
    "        i=i+1\n",
    "        \n",
    "    return prof\n",
    "\n",
    "# Compute time averages from profiles\n",
    "\n",
    "def make_time_averages(prof,period):\n",
    "    nt = prof['t'].size\n",
    "    nz = prof['z'].size\n",
    "    # compute the number of time averages we have in the series\n",
    "    navg = (nt//period)\n",
    "    ntot = navg * period\n",
    "    avg = {} # Average value during #period\n",
    "    start = {} # Start value at begining of #period\n",
    "    for key in prof.keys():\n",
    "        if prof[key].ndim == 2:\n",
    "            field = np.reshape(prof[key][:,:ntot],(nz,navg,period))\n",
    "            avg[key]=np.mean(field,axis=2)\n",
    "            start[key]=field[:,:,0]\n",
    "    start['t']=prof['t'][:ntot:period]\n",
    "    avg['t']=start['t']\n",
    "    avg['z']=prof['z']\n",
    "    start['z']=prof['z']\n",
    "    return avg,start"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc29e5da-ae3b-429b-a4a7-f912043b6389",
   "metadata": {},
   "source": [
    "We load Idefix output in a dictionnary named \"prof\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0941d15-d1e5-411e-91ce-2cc9d4b8be56",
   "metadata": {},
   "outputs": [],
   "source": [
    "prof=readProfile(\"timevol_2.dat\")\n",
    "prof.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d18ce61-6504-4f14-b335-6732469908be",
   "metadata": {},
   "source": [
    "As you can see, the dataset contains many properties, such as the density \"rho\", the velocity field \"vx\", \"vy\", \"vz\", etc. These are all horizontally averaged quantities, stored as functions of $z$ and $t$. Let's plot the density \"rho\", and the horizontal components of the magnetic field \"Bx\" and \"By\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa2edfd-2dd2-4d95-ae77-c3a4f8750503",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(13,4))\n",
    "plt.pcolormesh(prof['t'],prof['z'],np.log10(prof['rho']),cmap='gnuplot')\n",
    "plt.xlabel('t')\n",
    "plt.ylabel('z')\n",
    "plt.colorbar()\n",
    "plt.title(r'$\\rho$')\n",
    "\n",
    "plt.figure(figsize=(13,4))\n",
    "plt.pcolormesh(prof['t'],prof['z'],prof['bx'],cmap='seismic')\n",
    "plt.xlabel('t')\n",
    "plt.ylabel('z')\n",
    "plt.colorbar()\n",
    "plt.title('$B_x$')\n",
    "\n",
    "plt.figure(figsize=(13,4))\n",
    "plt.pcolormesh(prof['t'],prof['z'],prof['by'],cmap='seismic')\n",
    "plt.xlabel('t')\n",
    "plt.ylabel('z')\n",
    "plt.colorbar()\n",
    "plt.title('$B_y$')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "072bcb2e-8ce7-40ff-998a-23112688de45",
   "metadata": {},
   "source": [
    "In order to reduce the noise, we average our profile in a time window. In addition we compute the vertical gradient of the EMFs that will be useful later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1670dc3b-94b3-42c0-9618-a9441bcb2d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will make use of log(rho), so let's compute this now\n",
    "prof['logrho']=np.log10(prof['rho'])\n",
    "\n",
    "# Make some time average on a window\n",
    "period=50\n",
    "avg,start=make_time_averages(prof,period)\n",
    "\n",
    "dz=prof['z'][1]-prof['z'][0]\n",
    "avg['dzEx']=np.gradient(avg['Ex'],dz,axis=0)\n",
    "avg['dzEy']=np.gradient(avg['Ey'],dz,axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d96ae5e-d435-4437-881c-8657f6902ae4",
   "metadata": {},
   "source": [
    "Let's plot the time-average quantities, that you can compare to the raw outputs above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4956e4b-7ce1-4f26-8a54-8edb04bce536",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(13,4))\n",
    "plt.pcolormesh(avg['t'],avg['z'],avg['by'],cmap='seismic')\n",
    "plt.xlabel('t')\n",
    "plt.ylabel('z')\n",
    "plt.title('$B_y$')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb9be77-1642-421a-a5c2-569ace4e1f2a",
   "metadata": {},
   "source": [
    "## 2- Transform this into a dataset for a neural network\n",
    "\n",
    "### a- basic idea\n",
    "We want to reproduce the butterfly diagram using an \"effective\" subgrid model. For this, we will use the horizontally averaged induction equation\n",
    "$$\n",
    "\\partial_t \\langle B_x\\rangle =\\partial_z \\langle E_y\\rangle\n",
    "$$\n",
    "$$\n",
    "\\partial_t \\langle B_y\\rangle =-q\\Omega B_x-\\partial_z \\langle E_x \\rangle\n",
    "$$\n",
    "We will need a \"subgrid\" model to know what's happening to $E_x$ and $E_y$, with $E=\\langle \\delta v\\times\\delta B\\rangle$. \n",
    "\n",
    "We will make the hypothesis that $E_x(z)$ and $E_y(z)$ are functions of the values of $B_x,B_y,\\rho$ at $z$ and at the immediate neighbouring cells of $z$: $z+dz$ and $z-dz$. Hence, the input of our network (named $X$ in the following) will be $B_x,B_y,\\rho$ at $z-dz,z,z+dz$ and the outputs (named $Y$) will be $E_x,E_y$ at $z$.\n",
    "\n",
    "### b- create the input set $X$ and output set $Y$\n",
    "\n",
    "To avoid the initial transient of the simulations, we start sampling at $t_\\mathrm{min}=100$. We organise $X$ and $Y$ in samples $s$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda316fe-7286-42dd-9d12-a486c9111c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Xkeylist=['bx','by','logrho'] # the list of input variables with which we work \n",
    "Ykeylist=['Ex','Ey'] # the list of variables we ought to guess\n",
    "neighb=1 # number of neighbours in z\n",
    "tmin=100\n",
    "Xnvars=len(Xkeylist)\n",
    "Ynvars=len(Ykeylist)\n",
    "nt=np.sum(avg['t']>tmin)  # array size in t\n",
    "nt_start=np.argwhere(avg['t']>tmin)[0][0]   # index of the first item\n",
    "nz=avg['z'].size-2*neighb # remove the boundaries in z\n",
    "X_total=np.zeros([nt*nz,3*Xnvars]) # 3* for the cell center+left neighbour+right neighbour\n",
    "Y_total=np.zeros([nt*nz,Ynvars]) # \n",
    "nk=0 # number of the key\n",
    "\n",
    "# fill X\n",
    "for key in Xkeylist:\n",
    "    pcentered=avg[key][neighb:-neighb,nt_start:].flatten()\n",
    "    pm1=avg[key][0:-2*neighb,nt_start:].flatten()\n",
    "    pp1=avg[key][2*neighb:,nt_start:].flatten()\n",
    "    X_total[:,nk]=pcentered\n",
    "    X_total[:,nk+Xnvars]=pm1\n",
    "    X_total[:,nk+2*Xnvars]=pp1\n",
    "    nk=nk+1\n",
    "    \n",
    "# fill Y\n",
    "nk=0\n",
    "Yscaling=np.zeros([Ynvars,2])\n",
    "for key in Ykeylist:\n",
    "    pcentered=avg[key][neighb:-neighb,nt_start:].flatten()\n",
    "    Yscaling[nk,0]=np.mean(pcentered)  # Mean\n",
    "    Yscaling[nk,1]=np.sqrt(np.mean((pcentered-Yscaling[nk,0])**2)) # RMS\n",
    "    Y_total[:,nk]=pcentered\n",
    "    nk=nk+1\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71127baf-1c61-4830-a65a-27bc34dbf56d",
   "metadata": {},
   "source": [
    "Visualize the two components of the Y sample (i.e $E_x$ and $E_y$ of each sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e831be2f-18ca-4a6d-af42-586410851a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(Y_total[:,0])\n",
    "plt.title(\"$E_x$\")\n",
    "plt.figure()\n",
    "plt.plot(Y_total[:,1])\n",
    "plt.title(\"$E_y$\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcfe0566-edca-4589-8579-1bd2fe431eef",
   "metadata": {},
   "source": [
    "### c- Shuffle and split\n",
    "\n",
    "The data has retained the vertical structure of the flow (see figures above!), so we must shuffle this to make a random training set, and normalize it all so that $X$ and $Y$ all lies between $[-1,1]$. Note that we determine the normalisation for the training set, and we apply the *same* normalization to the validation and test sets!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c56c23cf-2ecc-43c7-9ef8-fea302794c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# Split the sets into a test and a \"full\" training set, picking randomly each sample\n",
    "X_train_full, X_test, Y_train_full, Y_test = train_test_split(X_total, Y_total, random_state=42)\n",
    "\n",
    "# Split the full training set into a training set and a validation set\n",
    "X_train, X_valid, Y_train, Y_valid = train_test_split(X_train_full, Y_train_full, random_state=42)\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train) # compute normalization and apply it\n",
    "X_valid = scaler.transform(X_valid)   # apply normalization\n",
    "X_test = scaler.transform(X_test)     # apply normalization\n",
    "\n",
    "scalerY = StandardScaler()            # new normalization for Y\n",
    "Y_train = scalerY.fit_transform(Y_train)\n",
    "Y_valid = scalerY.transform(Y_valid)\n",
    "Y_test = scalerY.transform(Y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d67fa16-ada7-47a8-bcf0-d4473d9b940b",
   "metadata": {},
   "source": [
    "Check that now our sets are properly normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1880f4c3-36d1-45ea-a85a-297bb59d94d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb0f3567-0f55-4aa2-9931-37b34ce1c6d8",
   "metadata": {},
   "source": [
    "# 3- Make a neural network!\n",
    "\n",
    "### a- import and initialize tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "196be523-03b1-408e-aac7-521491cc202e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3492470e-b609-4474-aadc-d5c322d05162",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init tensor flow\n",
    "keras.backend.clear_session() \n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a1e373c-36ee-487a-9a8d-eda814d966dc",
   "metadata": {},
   "source": [
    "### b- create a neural network\n",
    "\n",
    "The neural network will hae with 3 hidden layers, and a \"relu\" activation function in each layer. The final layer as two neurons, each corresponding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc5910d-cee6-4280-b598-86c09a7ee939",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(256, activation=\"relu\", input_shape=X_train.shape[1:]),\n",
    "    keras.layers.Dense(256, activation=\"relu\"),\n",
    "    keras.layers.Dense(128, activation=\"relu\"),\n",
    "    keras.layers.Dense(2)\n",
    "])\n",
    "model.compile(loss=\"mean_squared_error\", optimizer=keras.optimizers.Adam(learning_rate=1e-3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb4fbdb-9fe5-44c3-a3a1-51920c92455f",
   "metadata": {},
   "source": [
    "### c- train it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cce6303-0b2c-4806-8068-82a8c9e39da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train, Y_train, epochs=20, validation_data=(X_valid, Y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "494b42c0-e017-4e90-93a8-65329d13ea1b",
   "metadata": {},
   "source": [
    "### d- test it!\n",
    "\n",
    "Check how we're doing on the test set (note that the network has not seen the test data up to this point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c7f253d-8061-4e45-a564-473aaf74ad83",
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_test = model.evaluate(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4122b3e1-b14d-4fc6-b1e3-47e28856f28e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5fc62313-d82a-43bb-bcff-39c6f615e053",
   "metadata": {},
   "source": [
    "# 4- Use the neural network\n",
    "\n",
    "### a- Inference function\n",
    "\n",
    "Define a function to infer the EMFs given a state vector. A state is dictionnary assumed to contain $\\rho$ and $B_x$, $B_y$ along $z$ at a given instant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c0d08e-e51a-4654-81e4-fdf7576af0c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_Emf(model,state):\n",
    "    # construct the vector used by the model\n",
    "    # assumes state contains logrho and everything\n",
    "    # Create an input vector\n",
    "    X=np.zeros([state[Xkeylist[0]].shape[0]-2,9])\n",
    "    nvars=len(Xkeylist)\n",
    "    # fill X\n",
    "    nk=0 # number of the key\n",
    "    for key in Xkeylist:\n",
    "        pcentered=state[key][1:-1].flatten()\n",
    "        pm1=state[key][0:-2].flatten()\n",
    "        pp1=state[key][2:].flatten()\n",
    "        X[:,nk]=pcentered\n",
    "        X[:,nk+nvars]=pm1\n",
    "        X[:,nk+2*nvars]=pp1\n",
    "        nk=nk+1\n",
    "    \n",
    "    Xscaled = scaler.transform(X)\n",
    "\n",
    "    # Call the neural network to make a guess for Y\n",
    "    Yscaled = model.predict(Xscaled)\n",
    "\n",
    "    # invert Y according to the scaler used above\n",
    "    Y=scalerY.inverse_transform(Yscaled)\n",
    "    Ex=Y[:,0]\n",
    "    Ey=Y[:,1]\n",
    "    return Ex,Ey "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ed371d-3ee9-4856-96ab-056cdf23a0c2",
   "metadata": {},
   "source": [
    "### b- Use the inference function\n",
    "\n",
    "Here we show how to use the inference function using the simulations data at $t_\\mathrm{ref}=150$. We first build a state, call the get_emf function with the neural network and state, and finally plot the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca36298-4b77-4f86-a57d-b66f3496ab15",
   "metadata": {},
   "outputs": [],
   "source": [
    "tref=150\n",
    "state={}\n",
    "state['logrho']=avg['logrho'][:,tref]\n",
    "state['bx']=avg['bx'][:,tref]\n",
    "state['by']=avg['by'][:,tref]\n",
    "\n",
    "Ex,Ey=get_Emf(model,state)\n",
    "plt.figure()\n",
    "plt.plot(avg['z'],avg['Ex'][:,tref],label='Simulation')\n",
    "plt.plot(avg['z'][1:-1],Ex,label='Neural network')\n",
    "plt.title(r'$E_x$')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(avg['z'],avg['Ey'][:,tref],label='Simulation')\n",
    "plt.plot(avg['z'][1:-1],Ey,label='Neural network')\n",
    "plt.title(r'$E_y$')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57340cc3-0af6-4f66-a4fd-0c4840fa258e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
